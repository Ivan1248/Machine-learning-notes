\documentclass{article}
\usepackage[utf8]{inputenc}
\input{imports/font}
\input{imports/text}

\input{imports/math}
\input{imports/tables}
\input{imports/figures}
\input{imports/diagrams}
\input{imports/misc}
%\usepackage[sort&compress,round,comma,authoryear]{natbib}
\usepackage{natbib}




\title{Machine learning notes}
\author{}
\date{}

\input{imports/glossary}


\begin{document}

\maketitle

\tableofcontents
%\listoffigures
%\listoftables

\newpage

\begingroup
\onehalfspacing
\printunsrtglossary[type=symbols,style=supergroup,title={Notation}]
\endgroup




\section{Probability}

%http://www.workinginuncertainty.co.uk/probtheory_notation.shtml

\textbf{Probability of an event} $\P(E)$

The distribution of a random variable $\rvar x$ is denoted $\p_\rvar{x}$. If $\rvar x$ is known to be discrete, its distribution can be denoted with $\P_\rvar{x}$. 
\begin{align}
    \P_\rvar{x}(x) &= \P(\rvar x = x) \text.
\end{align}
\begin{align}
    \p_\rvar{x}(x) &= \lim_{\epsilon\to0}\frac{\P\del{\rvar x\in B_\epsilon(x)}}{\int_{x'\in B_\epsilon(x)}\dif x'} \text.
\end{align}
\begin{align}
    \P_\rvar{x}(x\in A) = \int_{x\in A}\dif \p_\rvar{x}(x) \text.
\end{align}

\subsection{Functions of random variables}

For any function $f\in(\set A\to\set B)$, we will use the same symbol to denote an equivalent function that maps random variables taking values in $\set A$ to random variables taking values in $\set B$:
\begin{align}
    \P(f(\rvar a)\in \set B_1) \coloneqq \P\del{\rvar a\in f^{-1}\sbr{\set B_1}} \text,
\end{align}
where $f^{-1}\sbr{\set B_1} \coloneqq \cbr{a\mid f(a)\in \set B_1}$ is what we call the preimage of $\set B_1\subseteq\set B$ under $f$.

\textbf{Conditional expectation}:
\begin{align}
    \E\del{\rvar x\mid\rvar y} = \del{y\mapsto \E\del{\rvar x\mid y}}(\rvar{y}) \text.
\end{align}
The conditional expectation $E\del{\rvar x\mid\rvar y}$ is a function of the random variable $\rvar y$ and, thus, is a random variable as well. 

\section{Statistics}

\subsection{Monte Carlo integration}

Monte Carlo integration (approximation) is a method of approximating integrals that can be expressed as expectations of some random variables.

Let $u\in \set A\to \R$ be a function. Let $I\coloneqq\int u(x)\dif x$ be an integral that is hard to compute. If we express $u$ as the product of a function $f$ and a probability density function (distribution) $p$, $u(x) = f(x)p(x)$, the integral $I$ can be expressed as the expectation of $f(\rvar x)$, where $\rvar x$ is distributed according to $p$:
\begin{align}
    I &= \int f(x)\dif x = \int f(x)p(x)\dif x = \E_{x\sim p}(f(x)) \text .
\end{align}
This expectation can be approximated with the following estimator:
\begin{align} 
    \rvar{\hat I}_n \coloneqq \frac{1}{n}\sum_{i=1\bidot n} f(\rvar x_i) \text{,}
\end{align}
where $\rvar x_i\sim p$. The estimator $\rvar{\hat I}_n$ is unbiased if $\rvar x_i$ are independent and it is valid if variances of $u(\rvar x_i)$ are bounded.

\subsection{Rejection sampling}

\subsection{Importance sampling}
$I\coloneqq\int_{x\in \set B}f(x)\dif x$

\section{Information theory}


\subsection{Information-theoretic measures}

functionals

\subsubsection{Basic concepts}

\textbf{Information content} of an event -- optimal message length for the event $\event{\rvar x=x}$:
\begin{align}
\I(\rvar x=x) = -\ln\P(x) \text.
\end{align}

\textbf{Entropy} (Shannon entropy) of a random variable (or a distribution) -- expected message length for optimally encoded elementary events of $\rvar x$:
\begin{align}
    \H(\rvar x) &= \E_{\rvar x}\I(\rvar x=x) = -\E\ln\P(\rvar x) \text.
\end{align}
The same formula applies for joint distributions, e.g.\ for the distribution $\P(\rvar x,\rvar y)$, we denote entropy (also called \textbf{joint entropy}) by $\H(\rvar x,\rvar y)$. Entropy is a common measure of uncertainty.

\textbf{Cross entropy} -- expected message length if the optimal code for $\P_\rvar{y}$ is used, but $\P_\rvar{x}$ is sampled.
\begin{align}
    \H_\rvar{y}(\rvar x) = \E_{\rvar x}\I(\rvar y=x) = -\E_{\rvar x}\ln\P(\rvar y=x) \text.
\end{align}

\textbf{Relative entropy} (Kullback–Leibler (KL) divergence) -- difference of cross entropy and entropy, measures how much $\P_\rvar{y}$ differs from $\P_\rvar{x}$:
\begin{align}
    \Dist_\rvar{y}(\rvar x) = \H_\rvar{y}(\rvar x) - \H(\rvar x) = \E_\rvar{x}\del{\I(\rvar y=x)-\I(\rvar x=x)} = \E_\rvar{x}\ln\frac{\P(x)}{\P(\rvar y=x)} \text.
\end{align}

\textbf{Mutual information} of random variables is the expectation of how much knowing the outcome of one of them gives information (or reduces the uncertainty) about the other:
\begin{align}
    \I(\rvar x; \rvar y) = \H(\rvar x) - \H(\rvar x\mid\rvar y) = \H(\rvar y) - \H(\rvar y\mid\rvar x) = \H(\rvar x) +\H(\rvar y) - \H(\rvar x, \rvar y) \text.
\end{align}
If there is a common condition, we can use $\I\del{\rvar x; \rvar y\mid z}$ as a shorter notation for $\I\del{(\rvar x\mid z); (\rvar y\mid z)}$. If we want to express mutual information between e.g.\ $\rvar x$ and $\rvar y\mid z$, we do it like this: $\I(\rvar x;(\rvar y\mid z))$, without ambiguity.

Mutual information can also be expressed as relative entropy:
\begin{align}
    \I(\rvar x; \rvar y) = \Dist_{\P_{\rvar x}\P_\rvar{y}}(\P_{\rvar x,\rvar y}) \\
    \I(\rvar x; \rvar y) = \Dist_{\P[\rvar x]\P[\rvar y]}(\P[\rvar x,\rvar y]) \\
    \I(\rvar x; \rvar y) = \Dist_{\P(\rvar x)\P(\rvar y)}(\P(\rvar x,\rvar y)) \\
    \I(\rvar x; \rvar y) = \Dist_{[\rvar x][\rvar y]}([\rvar x,\rvar y]) \text.
\end{align}

\subsubsection{Conditional measures}

Conditional counterparts of the information-theoretic measures have random variables in the condition-part of the expression that represents the argument of a measure. e.g.\ \textbf{Conditional entropy} is defined like this:
\begin{align}
    \H(\rvar x\mid\rvar y) &= \E_{\rvar y}\H(\rvar x\mid y) \text.
\end{align}
Similarly, conditional cross-entropy can be defined like this:
\begin{align}
    \H_\rvar{y}(\rvar x\mid\rvar z) &= \E_\rvar{z}\H_\rvar{y}(\rvar x\mid z) \text,
\end{align}
conditional mutual information like this:
\begin{align}
    \I(\rvar x; \rvar y\mid \rvar z) = \E_\rvar{z}\I\del{\rvar x; \rvar y \mid z} \text.
\end{align}

\subsubsection{Differential counterparts}

\subsubsection{Information theory and measure theory}

\url{https://en.wikipedia.org/wiki/Information_theory_and_measure_theory}

\subsection{Kolmogorov complexity}

\subsection{Minimum description length}


\section{Gradient-based optimization}

Notation, where $f\in(\set A\to\set B)$, $x\in \set A$, and $y \coloneqq f(x)$ (equivalently $f=(x\to y)$):
\begin{align}
    \Dif_f &= \pd{f(x)}{x} = f' &&\text{ is a function,} \\
    \Dif_f(x) &= \pd{f(\alpha)}{\alpha}(x) = \eval{\pd{f(\alpha)}{\alpha}}_{x} = \pd{f(x)}{x} = f'(x) &&\text{ is a domain element, } \\
    \Dif_{x\mapsto y} &= \pd{y}{x} = \pd{y}{x}(x') &&\text{ is a domain element (short), }
\end{align}

\subsection{Dependence of gradient on parametrization}

Let $\vec x$ be a vector in the input space of a differentiable function $f\in\del{\R^n\to\R}$. Let $\vec y \coloneqq \frac{1}{a}\vec x$. Then $f(a\vec y)=f(\vec x)$ and
\begin{align}
    \pd{f\del{a\vec y}}{\vec y} = a\pd{f(a\vec y)}{a\vec y} = a\pd{f(\vec x)}{\vec x} \text{.}  \label{eq:reparametrization-grad}
\end{align}

Using equation \eqref{eq:reparametrization-grad}, we can compare the ratio of the scale of the parameter and the scale of the gradient:
\begin{align}
    \frac{\enVert{\pd{f\del{a\vec y}}{\vec y}}}{\enVert{\vec y}}
    = \frac{\enVert{a\pd{f(\vec x)}{\vec x}}}{\enVert{\frac{1}{a}\vec x}}
    = a^2\frac{\enVert{\pd{f(\vec x)}{\vec x}}}{\enVert{\vec x}} \text{.}
\end{align}

In order to make the gradient invariant to such a parametrization, it could be divided by the square of its $p$-norm (or multiplied by the square of the norm of the parameter).
We can eaily show that the scaled gradient is independent of the parametrization:
\begin{align}
    \frac{\enVert{\pd{f\del{a\vec y}}{\vec y}}^{-2}\pd{f\del{a\vec y}}{\vec y}}{\enVert{\vec y}}
    = \frac{\enVert{a\pd{f\del{\vec x}}{\vec x}}^{-2}a\pd{f\del{\vec x}}{\vec x}}{\frac{1}{a}\enVert{\vec x}}
    = \frac{\enVert{\pd{f\del{\vec x}}{\vec x}}^{-2}\pd{f\del{\vec x}}{\vec x}}{\enVert{\vec x}} \text{.}
\end{align}
An issue that is introduced here is that the scaled gradient can't be $\cvec 0$. Moreover, it will grow as the unscaled gradient approaches $\cvec 0$, which is often undesirable. Alternatively, instead of dividing by the norm of its norm, the gradient could be multiplied by the square of the norm of the parameter:
\begin{align}
    \frac{\enVert{\vec y}^{2}\pd{f\del{a\vec y}}{\vec y}}{\enVert{\vec y}}
    = \frac{\enVert{\frac{1}{a}\vec x}^{2}a\pd{f\del{\vec x}}{\vec x}}{\frac{1}{a}\enVert{\vec x}}
    = \frac{\enVert{\vec x}^{2}\pd{f\del{\vec x}}{\vec x}}{\enVert{\vec x}} \text{.} \\
\end{align}
%!!IDEA!!: use this on each layers (or some) in backpropagation. Use L2 norm, and compensate for the number of features by using a factor making the norm scale-invariant. This should make training less dependent on the initialization scale. 

The gradient update rule for $\vec x$ is:
\begin{align}
    \vec x' = \vec x -\eta\pd{f(\vec x)}{\vec x}^\tp \text{.}
\end{align}
The gradient update rule for $\vec y$ is:
\begin{align}
    \vec y' 
    &= \vec y -\eta\pd{f(a\vec y)}{\vec y} ^\tp \\
    &= \frac{1}{a}\vec x -a\eta\pd{f(\vec x)}{\vec x}^\tp &&\text{(by equation \eqref{eq:reparametrization-grad})} \\
    &= \frac{1}{a}\del{\vec x -a^2\eta\pd{f(\vec x)}{\vec x}^\tp}\\
    &\neq \frac{1}{a}\del{\vec x -\eta\pd{f(\vec x)}{\vec x}^\tp} = \frac{1}{a}\vec x' \text{.}
\end{align}
We can see that, if the parameter $\vec x$ is scaled with $\frac{1}{a}$, the optimization step needs to be
multiplied with $\frac{1}{a^2}$. This discrepancy arises due to the property of the Jacobian being covariant to the input space transformation. Let's modify the gradient update rule with !!!inverting!!! the Jacobian:
\begin{align}
    \vec x' = \vec x -\eta\pd{\vec x}{f(\vec x)}^\tp \text{.}
\end{align}

\begin{align}
    \vec y' 
    &= \vec y - \eta\pd{\vec y}{f(a\vec y)} ^\tp \\
    &= \frac{1}{a}\vec x -\eta\pd{\frac{1}{a}\vec x}{f(\vec x)}^\tp \\
    &= \frac{1}{a}\vec x -\frac{1}{a}\eta\pd{\vec x}{f(\vec x)}^\tp \\
    &= \frac{1}{a}\del{\vec x -\eta\pd{\vec x}{f(\vec x)}^\tp} = \frac{1}{a}\vec x' \text{,}
\end{align}

\section{Machine learning}

\paragraph{An Occam's razor idea.} If the model (hypothesis search space) is simpler (smaller), we are more likely to find the correct hypothesis. If the correct hypothesis is complex, there will be more hypotheses consistent with the data and we are less likely to find the correct one anyway.


\section{Uncertainty in machine learning}

\subsection{Expressing uncertainty}

The basic and most complete way to express uncertainty are probability distributions. From a probability distribution, other uncertainty measures can be derived. Some common ones are the distribution of a derived random variable (a random variable which is a function of the original one), a parameter or a property of the distribution (e.g.\ the probability of the most certain value of the random variable or the entropy of the distribution).


\subsection{Epistemic and aleatory uncertainty}

%\paragraph{Što je epistemička nesigurnost?} 
\emph{Epistemička nesigurnost} (nesigurnost modela) je nesigurnost u model ili parametre. Ona se može smanjiti uz više podataka/informacija. \emph{Epistemička nesigurnost predikcije} dolazi od nesigurnosti u model/parametre.

%\paragraph{Kad možemo izraziti epistemičku nesigurnost?}
Kad parametre modela procjenjujemo točkasto, nemamo aposteriornu razdiobu parametara i ne znamo kakva je epistemička nesigurnost (ne možemo ju izraziti), ali ju možemo smanjiti uz više podataka. Kod bayesovske procjene parametara ili kod ansabla možemo procijeniti epistemičku nesigurnost.

%\paragraph{Što je aleatorna nesigurnost?}
\emph{Aleatorna nesigurnost} (predikcije) je nesigurnost koja dolazi od višeznačnosti podataka i ograničenja modela. Aleatorna nesigurnost se ne može smanjiti uz više podataka, ali bi se mogla smanjiti uz bolje podatke, tj. podatke koji imaju značajke koje sadrže više korisnih informacija, ili model koji pronalazi bolje značajke.

Kod diskriminativnog modela izlazna razdioba $p(y\mid x, \theta_\text{MAP})$ izražava aleatornu nesigurnost.

\paragraph{Je li ukupna nesigurnost zbroj epistemičke i aleatorne?}
Mislim da procjena ukupne nesigurnosti ovisi o tome koliko je dobro procijenjena epistemička nesigurnost. Što je lošija procjena aposteriorne razdiobe parametara, to je procjena epistemičke nesigurnosti lošija.


\subsection{Nesigurnost i izvanrazdiobni primjeri}

Neka je $D_{\text{train}}$ razdioba iz koje su došli primjeri za učenje. Diskrimanativni model uči funkciju $p(y\mid x)$. Ako je gustoća vjerojatnosti $D_{\text{train}}(x)$ jako mala (ili $0$), moguće je da nije bilo sličnih primjera u skupu za učenje i model može dati bilo kakvu predikciju za taj primjer. Takve primjeri su \emph{izvanrazdiobni primjeri}.

Ipak, pokazano je se da se (kod nekih modela) izvanrazdiobni primjeri često mogu dosta dobro prepoznavati na temelju izlazne razdiobe modela s točkasto procijenjenim parametrima. 


\subsection{Successfulness of epistemic uncertainty estimation with bayesian inference approximation}



\newpage
.
\newpage
\section{Adversarial examples and generalization}


Even though machine learning models achieve human-level performance in many tasks, when given out-of-distribution, domain-shifted, corrupted, or slightly modified examples from the training data distribution, they can often make overconfident and incorrect predictions \citep{Hendrycks:2016:BDMOODE,Ganin:2015:UDAB,Nguyen:2015:DNNEFHCPUI,Hendrycks:2019:BNNRCCP,Engstrom:2017:RTSFCST,Szegedy:2013:IPNN}. Perhaps most surprisingly, by perturbing input examples (e.g. images) even inperceptibly for humans, state-of-the-art models can be made to significantly change their predictions \citep{Szegedy:2013:IPNN, Goodfellow:2014:EHAE}. Such perturbed examples are called \textbf{adversarial examples}. The existence of adversarial examples indicates that such models are probably performing well for somewhat wrong reasons, without actually \textit{understanding data}.

\subsection{Adversarial example definitions}
A broadly accepted definition of an adversarial example is that it is an input designed to fool the model into producing an incorrect prediction.
This can be simplified to \textbf{adversarial example definition 1}:
\begin{quote}
    An input for which the model produces a misprediction.
\end{quote}
According to this definition, the set of adversarial examples is a function of the model (hypothesis) and the ground truth. What a misprediction is should also be defined. In classification, it is misclassification, i.e. the class with the highest probability being a wrong class. Another common definition is \textbf{adversarial example definition 2}: 
\begin{quote}
An input close to a natural input for which the model produces a different prediction.
\end{quote}
According to this definition, the set of adversarial examples also depends on the set of available natural examples and a similarity or distance function (or a neighbourhood function).

The second definition is useful for finding adversarial examples because we choose the neigbourhood function, but it is possible for a pair of natural examples with different ground truth labels to have overlapping neighbourhoods and to be adversarial examples of each other. 
%E.g. in classification, natural examples that are near the decision boundary can be within neighbourhoods of examples on the other side of the decision boundary, so even the true model (hypothesis) can have adversarial examples.
The first definition is more consistent, i.e. the true model has no adversarial examples. But then, knowing whether something is an adversarial example requires having solved the problem of finding the true model (which might be the problem we are trying to solve with the help of adversarial training).

There are also some different or broader definitions of adversarial examples. Some consider out-of-distribution examples producing high-confidence predictions \citep{Gal:2018:SCIMHNAETESBNN} or, most broadly, any inputs that fool the model \citep{Brown:2018:UAE}.

In the following text, adversarial examples in classification will be considered unless stated otherwise.

MANIFOLD

\subsection{Properties of adversarial examples.}



The existence of adversarial examples shows that near almost every input, there are misclassified inputs nearby. Evidence also shows that they are rare, i.e., they can not be easily found with random search in the neighbourhood of an natural example. This initially lead to the hypothesis that existence of adversarial examples is caused by high nonlinearity of deep models and that they are located in "low-probability pockets" in the data manifold \citep{Szegedy:2013:IPNN}.

However, \cite{Goodfellow:2014:EHAE} have found that it is enough to know the locally linear behaviour (e.g. gradient) with respect to the input to reliably generate adversarial examples along directions of increasing the linear approximation. They show that the direction of the perturbation matters more than the position in input space, evidencing against the low-probability pockets hypothesis. They propose the \textbf{linearity hypothesis}, according to which models are locally very linear. Thus, summing small perturbations in many elements of high-dimensional inputs can produce a large change in the prediction. \cite{Su:2017:OPAFDNN} have shown that even modifying only $1$ pixel can often be enough to cause a misclassification, indicating that models can be highly sensitive to some input elements.

An interesting property of adversarial examples is that adversarial examples generated for one model are often misclassified by other models and models trained on different datasets, i.e. they generalize across models and datasets \citep{Szegedy:2013:IPNN}. This property of adversarial examples is also called \textbf{transferability}.
\cite{Goodfellow:2014:EHAE, Tramer:2017:STAE} present some empirical evidence of transferability supporting the linearity hypothesis.



\subsection{Finding adversarial examples}

Let $\set X$ be the input space, and $d\in(\set X\times\set X\to \R^+)$ a \textbf{distance function} that can be used to define similarity between inputs. For each example $\vec x$, we can also define its \textbf{neighbourhood} as $B_{\epsilon}(\vec x) = \cbr{\vec x'\colon d(\vec x', \vec x) \leq \epsilon}$, $\epsilon$ being the maximum distance from the example.

Ideally, the neighbourhood of an example $\vec x$ should be the set of \textit{perceptually similar} examples that all belong to the same class as $\vec x$ (their true class may be at most ambiguous), but it is hard to define such a neighbourhood (as it requires knowing the true model). A practical and common way of defining the neighbourhood function (for images) is to let the distance function $d$ be a $L^p$ distance where $p$ is usually $\infty$ or $2$. Note that if an example is very near the true class boundary, such a neighbourhood may contain examples actually belonging to another class. 

Finding an adversarial example can be defined as an optimization problem of maximizing the loss with respect to the input with the constraint that the input is in the neighbourhood $B_\epsilon(\vec x)$:
\begin{align}
    \tilde{\vec x} = \argmax_{\vec x'\in B_\epsilon(\vec x)} L(y, h(\vec x')) , \label{eq:untargeted-loss-attack}
\end{align}
where $y$ is either the true label or the predicted label. Let $\hat{h}(\vec x) = \argmax_{y'} h(\vec x')_\ind{y'}$ denote the function that assigns the label with the highest probability to an input.
An objective can also be to find the $\tilde{\vec x}$ closest to $\vec x$ such that the classifier misclassifies it \citep{Moosavi-Dezfooli:2016:DFSAMFDNN}:
\begin{align}
    \tilde{\vec x} = \argmin_{\vec x'\colon \vec x'\in B_\epsilon(\vec x) \land \hat{h}(\vec x') \neq y} d(\vec x', \vec x). \label{eq:untargeted-closest-attack}
\end{align}

The described objectives, where it only matters that the adversarial example is misclassified, are objectives for \textbf{untargeted adversarial attacks}. There are also \textbf{targeted adversarial attacks}, where the objective is to create an adversarial example such that the model classifies it as some desired class. Targeted attack objectives corresponding to equations \eqref{eq:untargeted-loss-attack} and \eqref{eq:untargeted-closest-attack} are:
\begin{align}
    \tilde{\vec x} = \argmin_{\vec x'\in B_\epsilon(\vec x)} L(y_\text{t}, h(\vec x')) \label{eq:targeted-loss-attack}
\end{align}
and
\begin{align}
    \tilde{\vec x} = \argmin_{\vec x'\colon \vec x'\in B_\epsilon(\vec x) \land \hat{h}(\vec x') = y_\text{t}} d(\vec x', \vec x), \label{eq:targeted-closest-attack}
\end{align}
where $y_\text{t}$ denotes the adversarial target label.

Adversarial examples can also be generated without knowledge of the true label. Such adversarial examples are called \textbf{virtual adversarial examples}.
\cite{Miyato:2017:VATRMSSSL} propose the following objective for adversarial training:
\begin{align}
    \tilde{\vec x} = \argmin_{\vec x'\in B_\epsilon(\vec x)} D((\rvec y\mid \rvec x, \rvec\theta), (\rvec y\mid \rvec x = \vec x', \rvec\theta)),  
\end{align}
where $D$ is some non-negative function that represents distance between distributions. Other (untargeted) attacks can also produce virtual adversarial examples by using the predicted label $\hat{h}(\vec x)$ instead of the true label in the loss.

\subsection{Improving adversarial robustness.} There are different approaches (\textit{defenses}) trying to improve adversarial robustness, many of which have been shown to be ineffective on improved attacks \citep{Carlini:2017:AEANEDBTM,Athalye:2018:OGGFSS,Uesato:2018:ARDEAWA,Carlini:2017:TERNN}. A broad overview of many attacks can be found in \cite{Serban:2018:AECCP}. Some approaches are based on designing models which are less sensitive to small input perturbations with hard or soft constraints \cite{}, TODO
Currently, the best way to achieve \textbf{robustness} is adversarial training \citep{Goodfellow:2014:EHAE,Madry:2017:TDLMRAA}, where the model is trained to make correct predictions on adversarial examples. 

Nevertheless, empirical results show that improving robustness is a problem that requires more capacity and causes a reduction of performance of the model on natural data compared to traiing with natural examples only \citep{Madry:2017:TDLMRAA,Tsipras:2018:RMBOA} (atleast for deep models). All this will be discussed more thoroughly in the following sections.

\subsection{Adversarial examples and generalization}


\subsubsection{Transferability}

Common (naturally trained) CV models (algorithms) are biased similarly with respect to having adversarial examples -- they are non-robust in similar ways. 

Can this bias be easily overcome? Unfortunately, there seems not to be much evidence indicating this.

Overdependence on semantically low-level features.


\subsubsection{Robustness}

\cite{Athalye:2018:OGGFSS} state that, if there are classes that are very similar, using targeted attacks for robustness evaluation can give more meaningful results. 

stronger attack targeted, stronger evaluation untargeted

\subsubsection{Adversarial training}

\cite{Kurakin:2016:AMLS} note that by using the true label in the loss in untargeted attacks ($y$ in equation \eqref{eq:untargeted-loss-attack}) can cause


\newpage
.
\newpage
\subsubsection{Distance metrics for images}

Usually, an $L^p$ distance ($d(\vec x', \vec x)=\lVert\tilde{\vec x}-\vec x\rVert_p$) is used as a distance metric for adversarial examples.

\paragraph{Scale-invariant norms.}
Let $\vec x$ denote some image (or perturbation) and $\vec x_\lambda$ the same image with dimensions scaled by $\lambda$. $\vec x_\lambda$ has a greater norm because it contains $\lambda^2$ the number of pixels of the original image and every pixel is approximately effectively repeated $\lambda^2$ times. In terms of $\enVert{\vec x}_p$, its norm can be approximated like this:
\begin{align*}
    \enVert{\vec x_\lambda}_p 
    &= \del{\sum_{u\in\cbr{0\bidot\lambda H}}\sum_{v\in\cbr{0\bidot\lambda W}} \envert{{\vec x_\lambda}_\ind{u, v}}^p}^{\frac{1}{p}} && \text{(norm definition)} \\
    &\approx \del{\sum_{u\in\cbr{0\bidot H}}\sum_{v\in\cbr{0\bidot W}} \lambda^2 \envert{{\vec x}_\ind{u, v}}^p}^{\frac{1}{p}} && \text{($\lambda^2$ element copies)} \\
    &= \lambda^\frac{2}{p} \del{\sum_{u\in\cbr{0\bidot H}}\sum_{v\in\cbr{0\bidot W}} \envert{{\vec x}_\ind{u, v}}^p}^{\frac{1}{p}} \\
    &= \lambda^\frac{2}{p} \enVert{\vec x}_p \text{.} && \text{(norm definition)}
\end{align*}

If there is a perturbation in $2$ different resolutions, the higher-resolution one will have a greater norm by approximately a factor of $(\lambda^2)^\frac{1}{p}$, especially if the perturbations don't have too high spatial frequencies. Hence, scale invariant equivalents of $L^p$ norms can be defined by dividing the norm by the scale factor. If we choose the scale factor to be relative to the scale of an image with area $1$, $\lambda^2$ is $n$, the total number of pixels. 

We can define \textbf{scale-invariant norms} like this:
\begin{align}
    \mathrm{m}_p(\vec x) \coloneqq n^{-\frac{1}{p}}\enVert{\vec x}_p, \label{eq:scale-invariant-norm}
\end{align}
This can also be expressed as the \textbf{generalized mean} $\mathrm{M}_p$ (also known as \textbf{power mean}) of the elementwise absolute value:
\begin{align}
    \mathrm{m}_p(\vec x) = \mathrm{M}_p(\envert{\vec x}) \coloneqq \del{\frac{1}{n}\sum_i  \envert{\vec x_\ind{i}}^p}^{\frac{1}{p}}. \label{eq:generalized-mean}
\end{align}
Such norms could probably be useful for comparison of norms between different-resolution images and different datasets, and hyperparameter choice for adversarial attacks.

Maybe something similar could be done about objects of different scale?

...

\paragraph{Expectation of scale-invariant norms uniformly distributed random vectors.}
Let $\rvec x_n$ be a random vector with $n$ independent elements ${\rvec x_n}_\ind{i}\sim\mathcal{U}\del{\intcc{-\epsilon,\epsilon}}$.

An easy special case is when $p=1$:
\begin{align*}
    \E\sbr{\mathrm{m}_1(\rvec x_n)}
	= \E\sbr{\frac{1}{n}\sum_i \envert{{\rvec x_n}_\ind{i}}}
	= \E\sbr{\envert{{\rvec x_n}_\ind{i}}}  
	= \frac{\epsilon}{2} \text{.}
\end{align*}

For very large $n$, we can approximate it like this:
\begin{align*}
	\lim_{n\to\infty} \E \sbr{\mathrm{m}_p(\rvec x_n)}
	&= \E\sbr{\lim_{n\to\infty}\del{\frac{1}{n}\sum_i \envert{{\rvec x_n}_\ind{i}}^p}^\frac{1}{p}} &&\text{(dominated convergence)} \\
	&= \E\sbr{\E\sbr{\envert{{\rvec x_n}_\ind{0}}^p}^\frac{1}{p}} &&\text{(IID elements)}  \\
	&= \E\sbr{\envert{{\rvec x_n}_\ind{0}}^p}^\frac{1}{p} &&\text{(redundant expectation)}  \\
	&= \frac{1}{2\epsilon} \del{\int_{-\epsilon}^{\epsilon}\envert{x}^p\dif x}^\frac{1}{p} &&\text{(definition of expectation)}  \\	
	&= \frac{1}{\epsilon}\del{\int_{0}^{\epsilon}x^p\dif x}^\frac{1}{p} &&\text{(absolute value symmetry)}  \\	
	&= \frac{1}{\epsilon}\del{\frac{\epsilon^{p+1}}{p+1}}^\frac{1}{p} \text{.} \numberthis\label{eq:power-norm-n-infinity}
\end{align*}
Specially, for $\epsilon=1$, it evaluates to 
\begin{align}
	 \lim_{n\to\infty} \E \sbr{\mathrm{m}_p(\rvec x_n)} = (p+1)^{-\frac{1}{p}} \text{.}
\end{align}

TODO: Show that \eqref{eq:power-norm-n-infinity} is the upper bound if $p>1$ and the lower bound if $p<1$.

By the power mean inequality, which can be proved by applying the Jensen inequality, $\mathrm{m}_p(\rvec x)$ is monotonically increasing in $p$ (for $p>0$).


For easier analysis, let's first consider a special case: ${\rvec x_n}_\ind{i}\sim\mathcal{U}\del{\intcc{0,1}}$.
\begin{align*}
	\E \sbr{\mathrm{m}_p(\rvec x_n)}
	&= \E\sbr{\del{\frac{1}{n}\sum_i \envert{{\rvec x_n}_\ind{i}}^p}^\frac{1}{p}}  \\
	% not good (no dependence on n): &\approx \E\sbr{\del{\frac{1}{2p}}^\frac{1}{p}}  \\
	&= \idotsint_{\intcc{0,1}^n} \del{\frac{1}{n}\sum_i \envert{\vec x_\ind{i}}^p}^\frac{1}{p} \dif\vec x  \\
	&= n^{-\frac{1}{p}}\idotsint_{\intcc{0,1}^n} \del{\sum_i \envert{\vec x_\ind{i}}^p}^\frac{1}{p} \dif\vec x  
\end{align*}


\paragraph{Local $p$-norm and hiererchical norm.}
Let $\vec k$ denote a non-negative $2$-D kernel with $\enVert{\vec k}_1=1$, e.g. Gaussian cenetered at $(0,0)$. Let $\vec x$ denote an image perturbation and assume that it has a single channel for simplicity.
We can define the local $p$-norm around a pixel $(i, j)$ as
\begin{align}
    \mathrm{LocalNorm}_{p,\vec k}(\vec x)_\ind{i,j} = \del{\envert{\vec x}^p * \vec k}_\ind{i,j}^\frac{1}{p},
\end{align}
where the absolute value and powering operatons are elementwise.

We can then define a bi-level hierarchical $(p_1,p_2)$-norm as $\enVert{\mathrm{LocalNorm}_{p_1,\vec k}(\vec x)}_{p_2}$. This can be generalizad to a multi-level norm $(p_1,..,p_n)$-norm by chaining multiple local norms with potentially different kernels until the last, global, $p_n$-norm.

Why?

%Using e.g a $(1,\infty)$-norm for contraining adversarial perturbations could allow for more flexible perturbations locally, but having local changes not add up to the global norm.

%To make the analysis easier, we can consider $\vec x$ and $\vec x_\lambda$ to be continous functions with delta-peaks at the coordinates of pixels. 

%\begin{align}
%    \enVert{\vec x_\lambda}_p = \del{ \int_{u\in\intcc{0,\lambda h}\cap\N}\int_{v\in\intcc{0,\lambda w}\cap\N} {\vec x_\lambda}\del{u, v}}^{\frac{1}{p}}
%\end{align}

\subsection{Making adversarially robust classifiers}



\section{Generative adversarial networks}

\subsection{Getting the probability of the example from the generator}

first paragraph

case 1) normal generator

case 2) invertible generator


\section{Paper summaries}

\subsection{The Conditional Entropy Bottleneck (Anonymous, 2018)}

URL: \url{https://openreview.net/forum?id=rkVOXhAqY7}.


\section{p}

Neka je odnos između slučajnih varijabli $\rvar x$ i $\rvar y$ definiran funkcijom $f$ koja ishode jedne slučajne varijable deterministički preslikava u ishode druge, što označavamo ovako: $\rvar y = f(\rvar x)$.  Ako su $\rvar x$ i $\rvar y$ diskretne slučajne varijable, onda je razdioba slučajne varijable $\rvar y$ definirana ovako:
\begin{align}
	P_{\rvar y}(y) = \sum_{x\colon f(x)=y} P_\rvar{x}(x) \text{.}
\end{align} 
Ako su $\rvar x$ i $\rvar y$ kontinuirane slučajne varijable s vrijednostima iz $\R$ i $f$ je injektivna, može se pokazati \citep{Elezovic:2007:VSSV} da vrijedi
\begin{align} \label{eq:gustoca-funkcije-sv}
p_{\rvar y}(y) = p_\rvar{x}(x) \envert{\od{x}{y}} \text{.}
\end{align} 
Neka je $C_{\rvar x}(x) := \int_{-\infty}^{x} p_{\rvar x}(x') \dif{x'}$. Vrijednosti iz intervala $\intoo{x, x+\epsilon}$ na kojem je $f$ monotono rastuća preslikavaju se u interval $\intoo{f(x), f(x+\epsilon)}$. Granice su obrnute ako je $f$ monotono padajuća na tom intervalu. Budući da $\P\del{\rvar x\in\intoo{x, x+\epsilon}}=\P\del{\rvar y\in\intoo{f(x), f(x+\epsilon)}}$, vrijedi
\begin{align}
C_{\rvar x}(x+\epsilon)-C_{\rvar x}(x) = 
C_{\rvar y}(f(x+\epsilon))-C_{\rvar y}(f(x)) \text{.}
\end{align}
Ako obje strane jednadžbe dijelimo s $\epsilon$ i pustimo $\epsilon\to0$, 
\begin{align}
	\lim_{\epsilon\to 0}\frac{C_{\rvar x}(x+\epsilon)-C_{\rvar x}(x)}{\epsilon} = \lim_{\epsilon\to 0}\frac{C_{\rvar y}(f(x+\epsilon))-C_{\rvar y}(f(x))}{\epsilon} \text{.}
\end{align}
Redom, prema definiciji derivacije, pravilu derivacije složene funkcije i definiciji funkcija $C_\rvec{x}$ i $C_\rvec{y}$ kao integrala gustoće vjerojatnosti, slijedi:
\begin{align}
\od{}{x}C_{\rvar x}(x) &= \od{}{x}C_{\rvar y}(f(x)) \text{,}\\
\od{}{x}C_{\rvar x}(x) &= \od{}{f(x)}C_{\rvar y}(f(x))\od{}{x}f(x) \text{,}\\
p_{\rvar x}(x) &= p_{\rvar y}(f(x))\od{}{x}f(x) \text{.} \label{eq:gustoca-funkcije-sv-dokaz-rastuci}
\end{align}
Može se pokazati da je za monotono padajuće intervale desna strana jednadžbe~\eqref{eq:gustoca-funkcije-sv-dokaz-rastuci} pomnožena s $-1$, iz čega uz jednadžbu~\eqref{eq:gustoca-funkcije-sv-dokaz-rastuci} slijedi
\begin{align}
p_{\rvar x}(x) &= p_{\rvar y}(y)\envert{\od{y}{x}} \text{,} \label{eq:gustoca-funkcije-sv-dokaz-xy}
\end{align}
gdje je $f(x)$ zamijenjen s $y$. Množenjem toga s $\envert{\od{x}{y}}=\envert{\od{y}{x}}^{-1}$ slijedi jednadžba~\eqref{eq:gustoca-funkcije-sv}. To pravilo se može poopćiti i na vektore. Onda vrijedi \citep{Murphy:2012:MLPP}
\begin{align}
p_{\rvec y}(\vec y)=p_\rvec{x}(\vec x)\envert{\det\pd{\vec x}{\vec y}} \text{.}
\end{align}

Neka je $\rvar z$ zbroj slučajnih varijabli $\rvar x$ i $\rvar y$. Onda vrijedi
\begin{align}
	p_{\rvar z}(z) = \int p_{\rvar x,\rvar y}(x, z-x)\dif x \text{.}
\end{align}
Ako su $\rvar x$ i $\rvar y$ nezavisne, onda to postaje konvolucija:
\begin{align} \label{eq:nezavisne-gustoce-konvolucija}
p_{\rvar z}(z) = \int p_{\rvar x}(x)p_{\rvar y}(z-x)\dif x \eqqcolon (p_{\rvar x}*p_{\rvar y})(z) \text{.}
\end{align}

\subsection{PDF of vector r.v. defined via a function of a vector r.v.}

Let $f \in (\R^n\to\R^m)$ and $\rvec y = f(\rvec x)$. We want to compute the PDF of $\rvar y$, or, equivalently, the distribution $\p(\rvec y)$.
\begin{align}
    \pd{\rvec y}{\rvec x} \in \R^{m\times n}
\end{align}

For easier analysis, let's assume that $m=1$, i.e.\ $\rvec y$ is a scalar, and denote it with $\rvar y$. We want to compute its PDF.


\section{Dense anomaly detection for dense prediction based on reconstruction error}

Pretpostavljamo duboki diskriminativni model $h(\vec x;\vec\theta)$ s parametrima $\vec\theta$ koji ulaz $\vec x$ preslikava u vektor $\vec y$ koji predstavlja izlaznu razdiobu  $\p(\rvar y\mid \vec x, \vec\theta)$.

previsoka sigurnost (postizanje male pogreške na skupu za učenje, kalibracija temperaturnim skaliranjem)

kriva klasifikacija izvanrazdiobnih primjera

neprijateljski primjeri

\citep{Hendrycks:2016:BDMOODE}

\citep{Guo:2017:CMNN}

\citep{Lee:2017:TCCCDOOD}

\citep{Liang:2017:PDOODENN}

Neki pristupi za prepoznavanje anomalija/izvanrazdiobnih primjera (detaljnije opisati i s referencama):
\begin{itemize}
    \item iz predikcije -- očekujemo manju vjerojatnost i veću nesigurnost za izvanrazdioben primjere,
    \item iz neke skrivene reprezentacije -- možemo analizirati razdiobe logita ili nečega drugoga i pomoću toga propoznavati izvanrazdiobne primjere,
    \item eksplicitnim učenjem razlikovanja razdiobe skupa za učenje od neke pozadinske razdiobe,
    \item korištenje generativnog modela za generiranje primjera iz područja male gustoće vjerojatnosti i korištenje njih kao izvanrazdiobnih primjera
    \item korištenjem generativnog modela kod kojeg je moguće izračunati gustoću vjerojatnosti za primjer,
    \item korištenjem rekonstrukcijske pogreške autoenkodera.
\end{itemize}
Neki pristup ise mogu kombinirati.


\subsection{Autoencoders and GAN-s}

.


\subsection{Korištenje rekonstrukcijske pogreške autoenkodera za propoznavanje onoga što model ne zna da ne zna}

Pretpostavljamo duboki diskriminativni model $h(\vec x;\vec\theta)$ s parametrima $\vec\theta$ koji ulaz $\vec x$ preslikava u vektor $\vec y$ koji predstavlja izlaznu razdiobu  $\p(\rvar y\mid \vec x, \vec\theta)$.

\subsubsection{Korištenje autoenkodera za prepoznavanje izvanrazdiobnih primjera}

\cite{Sabokrou:2018:ALOCCND} za otkrivanje anomalija u slici predlažu korištenje autoenkodera (s jako velikom skrivenom reprezentacijom) kojemu se kod učenja kao ulaz daje zašumljena slika. Uz autoenkoder se dodaje diiskriminator koji se uči a razlikuje izlaz autoenkodera od stvarnih primjera za učenje. Kao gubitak se koristi težinski zbroj kvadratne rekonstrukcijske pogreške i suparničkog gubitka. Kao primjeri se koriste mali izrazani dijelovi većih slika. Za prepoznavanje anomalija koristi se izlaz diskriminatora za rekonstruirani primjer.

\cite{Pidhorskyi:2018:GPNDAA} isto predlažu pristup s autoenkoderom i superničkim gubitkom. Kod njih gubitak ima $3$ komponente: (1) suparnički gubitak koji potiče da primjeri za učenje "pokrivaju" cijelu zadanu (Gaussovu) razdiobu skrivene reprezentacije, (2) suparnički gubitak koji potiče da rekonstruirani primjeri budu iz razdiobe skupa za učenje (kao kod \cite{Sabokrou:2018:ALOCCND}) i (3) rekonstrukcijski gubitak. Kao mjera za procjenu je li primjer izvan razdiobe se koristi procjena $\p(\vec z\mid \set D)$ koja ovisi o udaljenosti od "manifolda". Trebam još pručiti kako se točno dobiva.

Pretpostavljamo duboki diskriminativni model $h(\vec x;\vec\theta)$ s parametrima $\vec\theta$ koji ulaz $\vec x$ preslikava u vektor $\vec y$ koji predstavlja izlaznu razdiobu  $\p(\rvar y\mid \vec x, \vec\theta)$. Želimo prepoznavati izvanrazdiobne primjere pomoću autoenkodera.

Neke ideje u vezi autoenkodera:
\begin{itemize}
    \item Koristiti dekoder s heteroskedastičkom \citep{Kendall:2017:WUNBDLCV} nesigurnošću u rekonstrukciju (modelirati $\p(\rvec x\mid \vec z)$) i $-\ln\p(\rvec x\mid \vec z)$ za empirijski gubitak.
    \item Isprobati rekonstrukciju neke skrivene reprezentacije klasifikatora kako bi se u rekonstrukcijskoj pogrešci naglasile značajke bitne za klasifikaciju (semantički bitne). Možemo $h$ rastaviti na dvije funkcije: $h(\vec x) = (f_2\circ f_1)(\vec x)$ pa onda učime autoenkoder rekonstruirati $f_1(\vec x)$. Ako kao kao $f_1$ koristimo bijekciju, možemo vidjeti kako izgleda rekonstrukcija ulaza koja odgovara rekonstruiranoj reprezentaciji.
    \item Isprobati klasifikaciju na temelju skrivene reprezentacije autoenkodera $\vec z$, koristiti i klasifikacijski gubitak za učenje kodera, vidjeti kako izgledaju rekonstrukcije. (Isprobati CEB?)
    \item Minimalna reprezentacija autoenkodera onemogućuje neprijateljske primjere kojima je cilj postići dobru rekonstrukciju anomalije, pogotovo ako pretpostavimo dovoljno dobru funkciju rekonstrukcijske pogreške ili diskriminator.
    \item Je li dobro poticati da skup za učenje pokriva cijelu razdiobu $\p(\rvec z)$? Onda će različite klase biti odmah jedna uz drugu -- malo izmijenimo $\vec z$ i dođemo u područje visoke gustoće za neku drugu klasu. Možda valja učiti razdiobu $\p(\rvec z\mid \set D)$ i znati koja su područja niže gustoće (margine) (kako?).
    \item Dodati šum na ulaz autoenkodera. Možda bi valjalo nešta između gaussovog šuma i "rupa" za popunjavanje.
\end{itemize}

Osnovni model koji bih htio isprobati (na velikim slikama) je ovakav:
\begin{alignat}{3}
    &\vec x \overset{f_1}{\longmapsto} \vec h \overset{f_2}{\longmapsto} \vec y 
    \quad&&\text{(klasifikator)} \text, \\
    &\vec h \overset{e}{\longmapsto} \vec z \overset{d}{\longmapsto} \vec h_\text{r}
    \quad&&\text{(autoenkoder skrivene reprezentacije)} \text.
\end{alignat}
Treba odrediti točan opis modela.

Možemo isprobati i klasifikaciju na temelju rekonstrukcije:
\begin{align}
    &\vec h_\text{r} \overset{f_2}{\longmapsto} \vec y\text.
\end{align}

Možemo isprobati i klasifikaciju na temelju skrivene reprezentacije:
\begin{align}
    &\vec z \overset{f_z}{\longmapsto} \vec y \text,
\end{align}
i istovremeno učenje klasifikacije i rekonstrukcije.

Bilo bi zanimljivo vidjeti kako izgleda rekonstrukcija ulazne slike na temelju izlaza autoenkodera ovisno o tome koji skriveni sloj se kodira:
\begin{align}
    &\vec h_\text{r} \overset{f_1^{-1}}{\longmapsto} \vec x_\text{r}
    \quad \text{(inverz prvog dijela klasifikatora s ulazom $\vec h_\text{r}$)} \text.
\end{align}
Rekonstrukciju ulazne slike možemo dobiti ako koristimo neki model koji je bijektivan, npr. i-RevNet.


\begin{align}
    \min_{h} L_\text{c}(\vec y, \vec y^*) \\
    \min_{d, e} L_\text{r}(\vec h,\vec h_\text{r})
\end{align}

\subsubsection{Što bih još htio isprobati}

Kombinaciju \cite{Lee:2017:TCCCDOOD} i korištenja izvanrazdiobnih primjera u učenju.

\bibliography{bibliography}
\bibliographystyle{plainnat} 
%\bibliographystyle{ieeetr}

\end{document}
